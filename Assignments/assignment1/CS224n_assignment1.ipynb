{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224n - assignment1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "YF6ONU8KG5U4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# q1_softmax"
      ]
    },
    {
      "metadata": {
        "id": "p4lG0FurG1w-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtjGcdgwG-1-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute the softmax function for each row of the input x.\n",
        "\n",
        "    It is crucial that this function is optimized for speed because\n",
        "    it will be used frequently in later code. You might find numpy\n",
        "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
        "    broadcasting useful for this task.\n",
        "\n",
        "    Numpy broadcasting documentation:\n",
        "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
        "\n",
        "    You should also make sure that your code works for a single\n",
        "    D-dimensional vector (treat the vector as a single row) and\n",
        "    for N x D matrices. This may be useful for testing later. Also,\n",
        "    make sure that the dimensions of the output match the input.\n",
        "\n",
        "    You must implement the optimization in problem 1(a) of the\n",
        "    written assignment!\n",
        "\n",
        "    Arguments:\n",
        "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
        "\n",
        "    Return:\n",
        "    x -- You are allowed to modify x in-place\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    x = e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "    assert x.shape == orig_shape\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F13KHrBSHAYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_softmax_basic():\n",
        "    \"\"\"\n",
        "    Some simple tests to get you started.\n",
        "    Warning: these are not exhaustive.\n",
        "    \"\"\"\n",
        "    print(\"Running basic tests...\")\n",
        "    test1 = softmax(np.array([1,2]))\n",
        "    print(test1)\n",
        "    ans1 = np.array([0.26894142,  0.73105858])\n",
        "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
        "\n",
        "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
        "    print(test2)\n",
        "    ans2 = np.array([\n",
        "        [0.26894142, 0.73105858],\n",
        "        [0.26894142, 0.73105858]])\n",
        "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
        "\n",
        "    test3 = softmax(np.array([[-1001,-1002]]))\n",
        "    print(test3)\n",
        "    ans3 = np.array([0.73105858, 0.26894142])\n",
        "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
        "\n",
        "    print(\"You should be able to verify these results by hand!\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DGFo6PdyHCy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "7aeca094-b83c-4b18-eda1-30d7b1e14561"
      },
      "cell_type": "code",
      "source": [
        "def test_softmax():\n",
        "    \"\"\"\n",
        "    Use this space to test your softmax implementation by running:\n",
        "        python q1_softmax.py\n",
        "    This function will not be called by the autograder, nor will\n",
        "    your tests be graded.\n",
        "    \"\"\"\n",
        "    print(\"Running your tests...\")\n",
        "    ### YOUR CODE HERE\n",
        "    test4 = softmax(np.array([[100, 100, 100],\n",
        "                             [100, 100, 100]]))\n",
        "    print(test4)\n",
        "    ans4 = np.array([[0.33333, 0.33333, 0.33333],\n",
        "                    [0.33333, 0.33333, 0.33333]])\n",
        "    assert np.allclose(test4, ans4, rtol=1e-05, atol=1e-06)\n",
        "\n",
        "    print(\"All tests passed!\\n\")\n",
        "    ### END YOUR CODE\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_softmax_basic()\n",
        "    test_softmax()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running basic tests...\n",
            "[0.26894142 0.73105858]\n",
            "[[0.26894142 0.73105858]\n",
            " [0.26894142 0.73105858]]\n",
            "[[0.73105858 0.26894142]]\n",
            "You should be able to verify these results by hand!\n",
            "\n",
            "Running your tests...\n",
            "[[0.33333333 0.33333333 0.33333333]\n",
            " [0.33333333 0.33333333 0.33333333]]\n",
            "All tests passed!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qrKJYamWHVPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# q2_sigmoid"
      ]
    },
    {
      "metadata": {
        "id": "r_ny9Um-Hd9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# libraries imported \n",
        "# import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-0iPpgSHnP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input here.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fjZ_1om6HpUM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid_grad(s):\n",
        "    \"\"\"\n",
        "    Compute the gradient for the sigmoid function here. Note that\n",
        "    for this implementation, the input s should be the sigmoid\n",
        "    function value of your original input x.\n",
        "\n",
        "    Arguments:\n",
        "    s -- A scalar or numpy array.\n",
        "\n",
        "    Return:\n",
        "    ds -- Your computed gradient.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    ds = s * (1 - s)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-F719O9ZHq5J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_sigmoid_basic():\n",
        "    \"\"\"\n",
        "    Some simple tests to get you started.\n",
        "    Warning: these are not exhaustive.\n",
        "    \"\"\"\n",
        "    print(\"Running basic tests...\")\n",
        "    x = np.array([[1, 2], [-1, -2]])\n",
        "    f = sigmoid(x)\n",
        "    g = sigmoid_grad(f)\n",
        "    print(f)\n",
        "    f_ans = np.array([\n",
        "        [0.73105858, 0.88079708],\n",
        "        [0.26894142, 0.11920292]])\n",
        "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
        "    print(g)\n",
        "    g_ans = np.array([\n",
        "        [0.19661193, 0.10499359],\n",
        "        [0.19661193, 0.10499359]])\n",
        "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
        "    print(\"You should verify these results by hand!\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Xz6tttMHtf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_sigmoid():\n",
        "    \"\"\"\n",
        "    Use this space to test your sigmoid implementation by running:\n",
        "        python q2_sigmoid.py\n",
        "    This function will not be called by the autograder, nor will\n",
        "    your tests be graded.\n",
        "    \"\"\"\n",
        "    print(\"Running your tests...\")\n",
        "    ### YOUR CODE HERE\n",
        "    # raise NotImplementedError\n",
        "    ### END YOUR CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "El7ywHVWHvgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0cbdad51-6502-4b4d-95c6-0898e9225573"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test_sigmoid_basic();\n",
        "    test_sigmoid()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running basic tests...\n",
            "[[0.73105858 0.88079708]\n",
            " [0.26894142 0.11920292]]\n",
            "[[0.19661193 0.10499359]\n",
            " [0.19661193 0.10499359]]\n",
            "You should verify these results by hand!\n",
            "\n",
            "Running your tests...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BAa2ugnOHHW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# q2_gradcheck"
      ]
    },
    {
      "metadata": {
        "id": "UWSL7guTHFcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# libraries imported \n",
        "# import numpy as np\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0ixLiecHMk-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# First implement a gradient checker by filling in the following functions\n",
        "def gradcheck_naive(f, x):\n",
        "    \"\"\" Gradient check for a function f.\n",
        "\n",
        "    Arguments:\n",
        "    f -- a function that takes a single argument and outputs the\n",
        "         cost and its gradients\n",
        "    x -- the point (numpy array) to check the gradient at\n",
        "    \"\"\"\n",
        "\n",
        "    rndstate = random.getstate()\n",
        "    random.setstate(rndstate)\n",
        "    fx, grad = f(x) # Evaluate function value at original point\n",
        "    h = 1e-4        # Do not change this!\n",
        "\n",
        "    # Iterate over all indexes ix in x to check the gradient.\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        # Try modifying x[ix] with h defined above to compute numerical\n",
        "        # gradients (numgrad).\n",
        "\n",
        "        # Use the centered difference of the gradient.\n",
        "        # It has smaller asymptotic error than forward / backward difference\n",
        "        # methods. If you are curious, check out here:\n",
        "        # https://math.stackexchange.com/questions/2326181/when-to-use-forward-or-central-difference-approximations\n",
        "\n",
        "        # Make sure you call random.setstate(rndstate)\n",
        "        # before calling f(x) each time. This will make it possible\n",
        "        # to test cost functions with built in randomness later.\n",
        "\n",
        "        ### YOUR CODE HERE:\n",
        "        x_0 = x[ix]\n",
        "\n",
        "        random.setstate(rndstate)\n",
        "        x[ix] = x_0 + h\n",
        "        f1 = f(x)[0]\n",
        "\n",
        "        random.setstate(rndstate)\n",
        "        x[ix] = x_0 - h\n",
        "        f2 = f(x)[0]\n",
        "\n",
        "        x[ix] = x_0\n",
        "        numgrad = (f1 - f2) / (2 * h)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        # Compare gradients\n",
        "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
        "        if reldiff > 1e-5:\n",
        "            print(\"Gradient check failed.\")\n",
        "            print(\"First gradient error found at index %s\" % str(ix))\n",
        "            print(\"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad))\n",
        "            return\n",
        "\n",
        "        it.iternext() # Step to next dimension\n",
        "\n",
        "    print(\"Gradient check passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Il1Wsc7AHPvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sanity_check():\n",
        "    \"\"\"\n",
        "    Some basic sanity checks.\n",
        "    \"\"\"\n",
        "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
        "\n",
        "    print(\"Running sanity checks...\")\n",
        "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
        "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
        "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "layJmrEfHRNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def your_sanity_checks():\n",
        "    \"\"\"\n",
        "    Use this space add any additional sanity checks by running:\n",
        "        python q2_gradcheck.py\n",
        "    This function will not be called by the autograder, nor will\n",
        "    your additional tests be graded.\n",
        "    \"\"\"\n",
        "    print(\"Running your sanity checks...\")\n",
        "    ### YOUR CODE HERE\n",
        "    # raise NotImplementedError\n",
        "    ### END YOUR CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8W-zj-IHScE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "77fe9ac9-b088-451d-acaf-09b7716e06a9"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sanity_check()\n",
        "    your_sanity_checks()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running sanity checks...\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "Gradient check passed!\n",
            "\n",
            "Running your sanity checks...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Ce4CRmyHz4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# q2_neural"
      ]
    },
    {
      "metadata": {
        "id": "_NPOjI4UHT_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# import numpy as np\n",
        "# import random\n",
        "\n",
        "# from q1_softmax import softmax\n",
        "# from q2_sigmoid import sigmoid, sigmoid_grad\n",
        "# from q2_gradcheck import gradcheck_naive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2iEPA2skH4nC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward_backward_prop(X, labels, params, dimensions):\n",
        "    \"\"\"\n",
        "    Forward and backward propagation for a two-layer sigmoidal network\n",
        "\n",
        "    Compute the forward propagation and for the cross entropy cost,\n",
        "    the backward propagation for the gradients for all parameters.\n",
        "\n",
        "    Notice the gradients computed here are different from the gradients in\n",
        "    the assignment sheet: they are w.r.t. weights, not inputs.\n",
        "\n",
        "    Arguments:\n",
        "    X -- M x Dx matrix, where each row is a training example x.\n",
        "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
        "    params -- Model parameters, these are unpacked for you.\n",
        "    dimensions -- A tuple of input dimension, number of hidden units\n",
        "                  and output dimension\n",
        "    \"\"\"\n",
        "\n",
        "    ### Unpack network parameters (do not modify)\n",
        "    ofs = 0\n",
        "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
        "\n",
        "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
        "    ofs += Dx * H\n",
        "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
        "    ofs += H\n",
        "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
        "    ofs += H * Dy\n",
        "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
        "\n",
        "    # Note: compute cost based on `sum` not `mean`.\n",
        "    ### YOUR CODE HERE: forward propagation\n",
        "    h = sigmoid(np.dot(X, W1) + b1)\n",
        "    y_ = softmax(np.dot(h, W2) + b2)\n",
        "    cost = - np.sum(labels * np.log(y_))\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    ### YOUR CODE HERE: backward propagation\n",
        "    dZ2 = y_ - labels\n",
        "    gradW2 = np.dot(h.T, dZ2)\n",
        "    gradb2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "    dh = np.dot(dZ2, W2.T)\n",
        "    dz = sigmoid_grad(h) * dh\n",
        "    gradW1 = np.dot(X.T, dz)\n",
        "    gradb1 = np.sum(dz, axis=0, keepdims=True)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    ### Stack gradients (do not modify)\n",
        "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
        "        gradW2.flatten(), gradb2.flatten()))\n",
        "\n",
        "    return cost, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tv5nrB69INc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sanity_check():\n",
        "    \"\"\"\n",
        "    Set up fake data and parameters for the neural network, and test using\n",
        "    gradcheck.\n",
        "    \"\"\"\n",
        "    print(\"Running sanity check...\")\n",
        "\n",
        "    N = 20\n",
        "    dimensions = [10, 5, 10]\n",
        "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
        "    labels = np.zeros((N, dimensions[2]))\n",
        "    for i in range(N):\n",
        "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
        "\n",
        "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
        "        dimensions[1] + 1) * dimensions[2], )\n",
        "\n",
        "    gradcheck_naive(lambda params:\n",
        "        forward_backward_prop(data, labels, params, dimensions), params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tX0MaX_5IQRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def your_sanity_checks():\n",
        "    \"\"\"\n",
        "    Use this space add any additional sanity checks by running:\n",
        "        python q2_neural.py\n",
        "    This function will not be called by the autograder, nor will\n",
        "    your additional tests be graded.\n",
        "    \"\"\"\n",
        "    print(\"Running your sanity checks...\")\n",
        "    ### YOUR CODE HERE\n",
        "    # raise NotImplementedError\n",
        "    ### END YOUR CODE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BCP0gTABIRk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63525b33-cbe2-404f-f339-86cd7fbeccf0"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sanity_check()\n",
        "    your_sanity_checks()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running sanity check...\n",
            "Gradient check passed!\n",
            "Running your sanity checks...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3XNjBLJ_ISuw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}